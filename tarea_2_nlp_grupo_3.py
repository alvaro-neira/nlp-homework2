# -*- coding: utf-8 -*-
"""Tarea 2  NLP -  Grupo 3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bHba_Qh55k2oHZ718CDwiqZQ-vTIwL3X

# Tarea 2: Procesamiento de Lenguaje Natural

Integrantes:

* Sebastián Alday (Rut: 18294095-K)
* Paula Canales G. (Rut: 18845155-1)
* Álvaro Neira R. (Rut: 13757209-5)
* Matias Rodriguez U. (Rut: 18362815-1)

# Introducción

El siguiente trabajo consiste en el desarrollo de un bot que conteste las preguntas de los clientes de una empresa de servicios. En este caso, se ha escogido la empresa Chilexpress. El bot 'Eva' ha sido entrenado de dos formas distintas, con un embedding (como Word2Vec o Elmo) y mediante un transformer.

# 1. Creación y limpieza de la base de datos

## 1.1 Elija una organización que cuenta con una lista de preguntas y respuestas

La compañía escogida para realizar este trabajo es Chilexpress. Su sección de preguntas y respuestas se muestra en el siguiente enlace:

Chilexpress
https://centrodeayuda.chilexpress.cl/home

## 1.2 Genere una tabla (tablaQA.xls) con las preguntas y respuestas de la organización seleccionada.

La tabla 'tablaQA.csv' consiste en una columna con pregunta y otra con respuestas. Como se observa en esta sección, la tabla consiste en 46 preguntas y sus respectivas respuestas.
"""

!wget https://raw.githubusercontent.com/alvaro-neira/nlp-homework2/main/tablaQA.csv -O /content/tablaQA.csv

import pandas as pd
qa = pd.read_csv('/content/tablaQA.csv', sep='\t')
qa.head()

qa.info()

"""## 1.3 Genere una tabla (tiposmensajes.xls) con ejemplos de las siguientes clases de mensaje: "saludo", "despedida", "nombre", "informacion"

La siguiente tabla corresponde a las preguntas de la sección QA de Chilexpress con la clase "información" y se han inventado los ejemplos de las clases "saludo", "despedida" y "nombre".
"""

!wget https://raw.githubusercontent.com/alvaro-neira/nlp-homework2/main/tiposmensajes.csv -O /content/tiposmensajes.csv

messages = pd.read_csv('/content/tiposmensajes.csv', sep=';')
messages.head()

"""## 1.4 Genere respuestas predeterminadas (respuestasDefecto.xls) para los mensajes de tipo "saludo", "despedida", "nombre"

Se generan respuestas por defecto para cada clase que hemos inventado, de esta forma, Eva puede responder a una pregunta de esta clase con una de las respuestas de esta lista.
"""

!wget https://raw.githubusercontent.com/alvaro-neira/nlp-homework2/main/respuestasDefecto.csv -O /content/respuestasDefecto.csv

answers = pd.read_csv('/content/respuestasDefecto.csv', sep=';')
answers.head()

"""## 1.5 Describa en términos generales las tablas que construyó.

*   Tabla QA: Se construyó en base a las preguntas y respuestas del centro de ayuda de Chilexpress https://centrodeayuda.chilexpress.cl/home . Se extrajo cada pregunta y respuesta de cada categoría (Envíos Nacionales, Envíos y retiros en App, Envíos internacionales, Giros en Chile y el mundo, Atención en sucursal, Cuenta empresa) y se eliminaron las preguntas repetidas. 
*   Tipos de mensajes: se construyó con mensajes tipo de saludo, despedida y nombre. Para la información se utilizó las preguntas obtenidas en la Tabla QA.
*   Respuestas por defecto: se construyó con respuestas tipo de saludo, despedida y nombre de nuestro bot Eva.

# 2. Análisis de distancia

Para el primer chatbot utilizará una métrica de su elección para responder
a cada pregunta/texto del usuario.

## 2.1 Utilice algún embeddings utilizado en el curso para codificar el texto de entrada

### 2.1.1 Word2Vec

El embedding utilizado para codificar el texto de entrada es Wor2Vec. Para ello, primero se preprocesa con un normalizador y un tokenizador.
"""

import nltk
import gensim
import re
import pandas as pd
import sklearn
import numpy as np

import sklearn.linear_model
import sklearn.model_selection
from sklearn.svm import LinearSVC
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

def normalizer(text): #normalizes a given string to lowercase and changes all vowels to their base form
    text = text.lower() #string lowering
    text = re.sub(r'[^A-Za-zñáéíóú]', ' ', text) #replaces every punctuation with a space
    text = re.sub('á', 'a', text) #replaces special vowels to their base forms
    text = re.sub('é', 'e', text)
    text = re.sub('í', 'i', text)
    text = re.sub('ó', 'o', text)
    text = re.sub('ú', 'u', text)
    return text

def preprocessor(text):
  text = normalizer(text)
  tokens = nltk.tokenize.casual_tokenize(text)
  #if len(tokens)==1:
  #  tokens=text
  return tokens

def vectorizer(text, model): #returns a vector representation from a list of words and a given model
    vectors = []
    for i in text:
        try:
            vectors.append(model.wv[i])
        except:
            pass
    return(np.nan_to_num(np.mean(vectors,axis=0)))

corpus = messages.Mensaje.tolist()
corpus_preprocessed = list(map(preprocessor,corpus))

word2vec_model = gensim.models.word2vec.Word2Vec(sentences = corpus_preprocessed, min_count=1)

"""## 2.2 Con el texto de entrada codificado y usando embeddings proponga una manera de identificar la clase del texto de entrada.

Se define la función para predecir la clase del texto de entrada.

### 2.2.1 Word2Vec

Se ajusta el modelo con los mensajes preprocesados.
"""

features = np.zeros(shape=(len(messages), word2vec_model.wv.vectors.shape[1]))
for i,msg in enumerate(messages.Mensaje):
  features[i,:] = vectorizer(preprocessor(msg), word2vec_model)

cv_results = sklearn.model_selection.cross_validate(sklearn.linear_model.LogisticRegression(max_iter=10000),features,messages.Clase)
cv_results["test_score"].mean()

def predict_class(question):
  tokens = preprocessor(question)
  vector = vectorizer(tokens, word2vec_model).reshape(1, -1)
  return clf.predict(vector)

clf = make_pipeline(StandardScaler(),LinearSVC(random_state=0, tol=1e-5))
clf.fit(features,np.array(messages.Clase))

"""Ejemplo de las preguntas y las clases obtenidas para cada una de ellas con el modelo. """

print(predict_class('¿Sólo las empresas pueden tener una cuenta con Chilexpress?'))
print(predict_class('Mi nombre es'))
print(predict_class('hola buenas'))
print(predict_class('hola'))
print(predict_class('¿puedo retirar un giro de dinero?'))
print(predict_class('Puede otra persona retirar mi pedido'))
print(predict_class('hasta luego'))
print(predict_class('adios'))

"""## 2.3 Si el texto de entrada es del tipo "información”, busque ahora la pregunta mas similar y retorne la respuesta asociada.

Primero, se preprocesa el texto de la misma forma que en el punto anterior, pero se agrega la eliminación de stopwords.
"""

import nltk

nltk.download('punkt')
nltk.download('stopwords')

pattern = r"""(?x)                   # set flag to allow verbose regexps
              (?:[A-Z]\.)+           # abbreviations, e.g. U.S.A.
              |\$?\d+(?:[.,]\d+)?%?  # numbers, incl. currency and percentages
              |\w+(?:[-']\w+)*       # words w/ optional internal hyphens/apostrophe
              |(?:[+/\-@&*¡!.,])     # special characters with meanings
            """

def normalizer(text, remove_tildes = True): #normalizes a given string to lowercase and changes all vowels to their base form
    text = text.lower() #string lowering
    text = re.sub(r'[^A-Za-zñáéíóú]', ' ', text) #replaces every punctuation with a space
    text = nltk.regexp_tokenize(text, pattern)
    text = ' '.join(text)
    if remove_tildes:
        text = re.sub('á', 'a', text) #replaces special vowels to their base forms
        text = re.sub('é', 'e', text)
        text = re.sub('í', 'i', text)
        text = re.sub('ó', 'o', text)
        text = re.sub('ú', 'u', text)
    return text

nltk_stopwords = nltk.corpus.stopwords.words('spanish')
stopwords_normalized = [normalizer(word) for word in nltk_stopwords]

def no_stopwords(processed):
  '''
  Función para eliminar las stopwords de un texto
  '''
  result = []
  for w in processed:
    if w not in stopwords_normalized:
      result.append(w)

  return result

corpus_question = qa.Pregunta.tolist()
questions_processed = []
for question in corpus_question:
  pre = preprocessor(question)
  pre = no_stopwords(pre)
  questions_processed.append(' '.join(pre))

"""Ejemplo de las preguntas preprocesadas."""

questions_processed[:5]

"""En esta sección se realiza una modificación del modelo Word2Vec para utilizar Doc2Vec para identificar la pregunta en la base de datos que sea más similar a la que se ha preguntado.

Referencias: 
* https://www.linkedin.com/pulse/how-find-semantic-similar-sentences-from-your-dataset-jayaraman/
* https://radimrehurek.com/gensim/models/doc2vec.html
"""

# <1 min
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize

tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(questions_processed)]

max_epochs = 2000

doc2_model = Doc2Vec(
    vector_size=150,
    alpha=0.001, 
    min_alpha=0.002,
    dm=1,
    window=2,
    min_count=1,
    workers=4
)
doc2_model.build_vocab(tagged_data)

# Train the model based on epochs parameter
for epoch in range(max_epochs):
    doc2_model.train(tagged_data,
                total_examples=doc2_model.corpus_count,
                epochs=doc2_model.epochs)

# Save model. 
doc2_model.save("similar_sentence.doc2_model")

"""Se crea una función get_answer que entrega la respuesta asociada a la predicción con el modelo doc2vec. Recibe tres parámetros:
* question: la pregunta que se espera obtener la respuesta. (requerida)
* topn: top n mejores resultados encontrados. (opcional)
* debug: Verdadero para imprimir más detalles de los resultados. (opcional)
"""

doc2_model= Doc2Vec.load("similar_sentence.doc2_model")

def get_answer(question, topn=1, debug=False):
  tokens = preprocessor(question)
  tokens = no_stopwords(tokens)
  vector = doc2_model.infer_vector(tokens)
  most_similar = doc2_model.docvecs.most_similar([vector], topn=topn)
  list_answers = qa.Respuesta.tolist()
  top = 1
  for index,value in most_similar:
    log = """
    --Top {}------------------------------
      pregunta similar    : {}
      respuesta asosciada : {}
      valor similitud     : {}
    --------------------------------------
    """.format(
        top,
        corpus_question[int(index)],
        list_answers[int(index)],
        value
    )
    if debug:
      print(log)
    else:
      print(list_answers[int(index)])
    top += 1

"""Algunos ejemplos de las preguntas realizadas y las 3 primeras identificaciones en la base de datos (pregunta-respuesta-valor). """

get_answer('¿Qué es la Orden de Transporte (OT)?', 3, True)

get_answer('¿Puedo solicitar materiales telefónicamente?', 3, True)

get_answer('¿Qué es un envío sobredimensionado?', 3, True)

get_answer('¿Puedo cobrar un giro en cualquier sucursal de Chilexpress?', 3, True)

"""## 2.4 Reporte el resultado con textos de prueba.

Se indican los siguientes ejemplos utilizando el modelo Doc2Vec (modificación de Word2Vec).
"""

text = 'puedo cambiar la dirección de envío?'
get_answer(text)

text = 'como hago seguimiento?'
get_answer(text)

text = 'quiero retirar una encomienda a otra sucursal'
get_answer(text)

text = 'puede ir otra persona a buscar mi pedido'
get_answer(text)

text = 'como retiro mi envío por pagar'
get_answer(text)

"""# 3. Análisis transformer

Para el segundo chatbot utilice un transformer (por ejemplo BERT).
"""

!pip install transformers
from transformers import pipeline, AutoTokenizer, Trainer, TrainingArguments, AutoModel, AutoModelForSequenceClassification

"""## 3.1 Utilice el modelo transformer para clasificar el texto de entrada, y para extraer la respuesta de la tabla de preguntas y respuestas cuando el mensaje sea del tipo “información”.

### 3.1.1 Clasificador

En la presente sección se entrena un clasificador con una modificación de BERT en español: BETO. Se han utilizado los pesos preentrenados de BETO desarrollados por el DCC para el tokenizador. 

ref: https://benjad.github.io/2020/08/04/clasificador-sentimiento-BERT/
"""

import torch
from transformers import  BertTokenizer
from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader, SequentialSampler
from transformers import BertForSequenceClassification, AdamW
from transformers import get_linear_schedule_with_warmup
from sklearn.model_selection import train_test_split
import torch.optim
import numpy as np
import pandas as pd
import time
import datetime
import random
from sklearn.metrics import confusion_matrix

# Select cpu or cuda
run_on = 'cpu'
device = torch.device(run_on)

# Load the dataset into a pandas dataframe.
messages['class'] = messages['Clase'].map({'información':0, 'nombre':1, 'saludo':2, 'despedida':3,})
clase = messages['class']
mensaje = messages['Mensaje']

# Split dataset
X_train, X_val, y_train, y_val = train_test_split(mensaje, 
clase, stratify=clase, test_size=0.2, random_state=42)

# Report datasets lenghts
print('Training set length : {}'.format(len(X_train)))
print('Validation set length : {}'.format(len(X_val)))

# Commented out IPython magic to ensure Python compatibility.
# < 2 min
# Tokenization  
# %cd /content
!pip install urllib3==1.25.10
!rm -rf transformers
!git clone https://github.com/huggingface/transformers \
&& cd transformers \
&& git checkout a3085020ed0d81d4903c50967687192e3101e770

!pip install transformers
!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/pytorch_weights.tar.gz -O pytorch_weights.tar.gz
!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/vocab.txt -O vocab.txt
!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/config.json -O config.json
!tar -xzvf pytorch_weights.tar.gz
!mv config.json pytorch/.
!mv vocab.txt pytorch/.

tokenizer = BertTokenizer.from_pretrained("pytorch/", do_lower_case=True)

def preprocessing(dataset):
    input_ids = []
    attention_mask = []
    for doc in dataset:
        encoded_doc = tokenizer.encode_plus(doc,
                   add_special_tokens=True, max_length=115,
                   truncation=True,pad_to_max_length=True)
        input_ids.append(encoded_doc['input_ids'])
        attention_mask.append(encoded_doc['attention_mask'])
    return (torch.tensor(input_ids),
           torch.tensor(attention_mask))

# Apply preprocessing to dataset
X_train_inputs, X_train_masks = preprocessing(X_train)
X_val_inputs, X_val_masks = preprocessing(X_val)

# Report max n° tokens in a sentence
max_len = max([torch.sum(sen) for sen in X_train_masks])
print('Max n°tokens in a sentence: {0}'.format(max_len))

"""Para el entrenamiento se utilizan batches de tamaño 2. El modelo corresponde a 'BertForSequenceClassification' y el optimizador es de tipo 'AdamW'.  """

# Data loaders
batch_size = 2

y_train_labels = torch.tensor(y_train.values)
y_val_labels = torch.tensor(y_val.values)


def dataloader(x_inputs, x_masks, y_labels):
    data = TensorDataset(x_inputs, x_masks, y_labels)
    sampler = SequentialSampler(data)
    dataloader = DataLoader(data, sampler=sampler,
                 batch_size=batch_size,
                 num_workers=0)
    return dataloader

train_dataloader = dataloader(X_train_inputs, X_train_masks,
                   y_train_labels)
val_dataloader = dataloader(X_val_inputs, X_val_masks, 
                 y_val_labels)

# set random seed
def set_seed(value):
    random.seed(value)
    np.random.seed(value)
    torch.manual_seed(value)
    torch.cuda.manual_seed_all(value)
set_seed(42)

# Create model and optimizer
model = BertForSequenceClassification.from_pretrained(
        "pytorch/", num_labels=4, output_attentions=False,
         output_hidden_states=False)

optimizer = AdamW(model.parameters(),
                  lr = 4e-5,
                  eps = 1e-6
                  )

if run_on == 'cuda':
    model.cuda()

# Define number of epochs
epochs = 3

total_steps = len(train_dataloader) * epochs

# Create the learning rate scheduler.
scheduler = get_linear_schedule_with_warmup(optimizer,
            num_warmup_steps = 0, 
            num_training_steps = total_steps)

#fuction to format time
def format_time(elapsed):
    elapsed_rounded = int(round((elapsed)))
    return str(datetime.timedelta(seconds=elapsed_rounded))

#function to compute accuracy
def flat_accuracy(preds, labels):
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return np.sum(pred_flat == labels_flat) / len(labels_flat)

"""Desarrollo del entrenamiento. """

# ~3 min
#function to train the model
def training(n_epochs, training_dataloader,
             validation_dataloader):
    # ========================================
    #               Training 
    # ========================================
    print('======= Training =======')
    for epoch_i in range(0,n_epochs):
        # Perform one full pass over the training set
        print("")
        print('======= Epoch {:} / {:} ======='.format(
             epoch_i + 1, epochs))
        # Measure how long the training epoch takes.
        t0 = time.time()
        # Reset the total loss for this epoch.
        total_loss = 0
        # Put the model into training mode.
        model.train()
        # For each batch of training data
        for step, batch in enumerate(training_dataloader):
            batch_loss = 0
            # Unpack this training batch from dataloader
            #   [0]: input ids, [1]: attention masks, 
            #   [2]: labels
            b_input_ids,b_input_mask, b_labels = tuple(
                                t.to(device) for t in batch)

            # Clear any previously calculated gradients
            model.zero_grad()

            # Perform a forward pass 
            outputs = model(b_input_ids,
                            token_type_ids=None,
                            attention_mask=b_input_mask,
                            labels=b_labels)

            # pull loss value out of the output tuple
            loss = outputs[0]
            batch_loss += loss.item()
            total_loss += loss.item()

            # Perform a backward pass 
            loss.backward()

            # Clip the norm of the gradients to 1.0.
            torch.nn.utils.clip_grad_norm_(model.parameters(),
                                            1.0)

            # Update parameters
            # ¿take a step using the computed gradient
            optimizer.step()
            scheduler.step()

            print('batch loss: {0} | avg loss: {1}'.format(
                  batch_loss, total_loss/(step+1)))
        # Calculate the average loss over the training data.
        avg_train_loss = total_loss / len(train_dataloader)


        print("")
        print("  Average training loss: {0:.2f}".
             format(avg_train_loss))
        print("  Training epoch took: {:}".format(
              format_time(time.time() - t0)))

        # ========================================
        #               Validation
        # ========================================
        # After the completion of each training epoch, 
        # measure accuracy on the validation set.

        print("")
        print("======= Validation =======")

        t0 = time.time()

        # Put the model in evaluation mode
        model.eval()

        # Tracking variables
        eval_loss, eval_accuracy = 0, 0
        all_logits = []
        all_labels = []
        # Evaluate data for one epoch
        for step, batch in enumerate(validation_dataloader):
            # Add batch to device
            # Unpack this training batch from our dataloader.
            #   [0]: input ids, [1]: attention masks,
            #   [2]: labels
            b_input_ids, b_input_mask, b_labels = tuple(
                                t.to(device) for t in batch)


            # Model will not to compute gradients
            with torch.no_grad():
                # Forward pass 
                # This will return the logits 
                outputs = model(b_input_ids,
                                token_type_ids=None,
                                attention_mask=b_input_mask)

            # The "logits" are the output values 
            # prior to applying an activation function 
            logits = outputs[0]

            # Move logits and labels to CPU
            logits = logits.detach().cpu().numpy()
            b_labels = b_labels.to('cpu').numpy()

            # Save batch logits and labels 
            # We will use thoses in the confusion matrix
            predict_labels = np.argmax(
                             logits, axis=1).flatten()
            all_logits.extend(predict_labels.tolist())
            all_labels.extend(b_labels.tolist())

            # Calculate the accuracy for this batch
            tmp_eval_accuracy = flat_accuracy(
                                logits, b_labels)
            # Accumulate the total accuracy.
            eval_accuracy += tmp_eval_accuracy

        # Report the final accuracy for this validation run.
        print("  Accuracy: {0:.2f}".
              format(eval_accuracy / (step+1)))
        print("  Validation took: {:}".format(
             format_time(time.time() - t0)))

    #print the confusion matrix"
    conf = confusion_matrix(
           all_labels, all_logits, normalize='true')
    print(conf)
    print("")
    print("Training complete")

#call function to train the model
training(epochs, train_dataloader, val_dataloader)

"""### 3.1.2 Extraccion de respuestas para el caso "informacion"

* Se copia la sección Q&A de la clase práctica de transformers. Es decir, el modelo de **huggingface**:
[distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es](https://https://huggingface.co/mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es) 
con pipeline: **question-answering**.
"""

# Transformer QA
beto_qa = pipeline('question-answering', 
        model='mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es',
        tokenizer=('mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es', {"use_fast": False})
    )

"""* Para el texto del contexto, simplemente, se concatenan todas las respuestas de testQA.csv y se genera un solo string:"""

context_text = ""
for index, row in qa.iterrows():
  context_text = context_text + ". " + row['Respuesta']

"""* La función **beto_get_answer()** se utiliza para entregar una respuesta mas amigable al usuario en caso de estar utilizando un chat bot real y que no encuentre una respuesta adecuada."""

def beto_get_answer(question):
  respuesta = beto_qa({'question': question, 'context': context_text})
  if respuesta['score'] < 0.01:
    return "Perdón, pero no tengo respuesta para esa pregunta."
  else:
    return respuesta['answer']

"""## 3.2 Reporte el tipo de red, y las métricas de entrenamiento usadas

### 3.2.1

* Para la sección 3.1.1 (BertForSequenceClassification) se entrenó el modelo con los pesos preentrenados de BETO (BERT para español) en el tokenizador. La métrica utilizada para optimizar el clasificador fue accuracy. Se obtuvo un accuracy de 0.97.- En general, identifica todos los textos correspondientes a 'información', 'nombre' y 'despedida', y obtiene 0.9 de accuracy para la clase 'saludo'.

### 3.2.2

* Para la seccion 3.1.2 (Q&A) se copió lo visto en la clase práctica de transformers. Es decir, el modelo de **huggingface**:\
[distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es](https://https://huggingface.co/mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es), consistente en:


*   BETO (BERT para español)
*   SQuAD 2.0
*   Aplicada *Distillation*

## 3.3 Reporte el resultado con los textos de prueba.
"""

print('¿Sólo las empresas pueden tener una cuenta con Chilexpress?')
print(beto_get_answer('¿Sólo las empresas pueden tener una cuenta con Chilexpress?')+"\n")
print('¿puedo retirar un giro de dinero?')
print(beto_get_answer('¿puedo retirar un giro de dinero?')+"\n")
print('Puede otra persona retirar mi pedido')
print(beto_get_answer('Puede otra persona retirar mi pedido')+"\n")
print('¿Qué es la Orden de Transporte (OT)?')
print(beto_get_answer('¿Qué es la Orden de Transporte (OT)?')+"\n")
print('¿Qué es un envío sobredimensionado?')
print(beto_get_answer('¿Qué es un envío sobredimensionado?')+"\n")
print('¿Quién ganará la próxima elección?')
print(beto_get_answer('¿Quién ganará la próxima elección?')+"\n")

# ~8min
stats_beto_qa = []
for index, row in qa.iterrows():
  res=beto_qa({'question': row['Pregunta'], 'context': context_text})
  stats_beto_qa.append({'question': row['Pregunta'], 'score':res['score'], 'answer':res['answer']})

def get_avg_score(stats):
  total=0.0
  n=0
  for ans in stats:
    n=n+1
    total = total + ans['score']
  return total/n

def show_as_table(stats):
  table_beto_qa = []
  for ans in stats:
    table_beto_qa.append([ans['question'], str(round(ans['score']*100.0, 2)), ans['answer']])
  table_beto_qa.append(['avg', str(round(get_avg_score(stats)*100.0, 2)), ans['answer']])
  pd.DataFrame(table_beto_qa, columns=['Pregunta', 'Score', 'Respuesta'])

show_as_table

"""# Conclusión

El desarrollo de esta tarea permite concluir que los métodos anteriores a transformers, en los que se utilizan embeddings como Word2Vec (desarrollado en esta tarea) o Elmo, tiene una mayor dificultad lograr entrenar un modelo que pueda identificar el tipo de preguntas o cuáles son las palabras claves de ellas. Se necesita una gran cantidad de ejemplos clasificados para lograr buenos resultados. 

En el caso de las redes tipo transformers, se utiliza un modelo preentrenado como BERT o BETO (BERT en español) como tokenizador. De esta forma, se utiliza un modelo entrenado en *todo* el idioma con lo que se pueden identificar las clases de las frases con una mayor precisión, con la misma cantidad de datos. 
"""

